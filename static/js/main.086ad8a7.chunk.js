(this["webpackJsonpophthalmology-ai"]=this["webpackJsonpophthalmology-ai"]||[]).push([[0],{71:function(e){e.exports=JSON.parse('[{"id":1,"title":"Prediction of cardiovascular risk factors from retinal fundus photographs via deep learning","url":"https://www.nature.com/articles/s41551-018-0195-0","download-url":"https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46425.pdf","abstract":"Traditionally, medical discoveries are made by observing associations, making hypotheses from them and then designing and running experiments to test the hypotheses. However, with medical images, observing and quantifying associations can often be difficult because of the wide variety of features, patterns, colours, values and shapes that are present in real data. Here, we show that deep learning can extract new knowledge from retinal fundus images. Using deep-learning models trained on data from 284,335 patients and validated on two independent datasets of 12,026 and 999 patients, we predicted cardiovascular risk factors not previously thought to be present or quantifiable in retinal images, such as age (mean absolute error within 3.26 years), gender (area under the receiver operating characteristic curve (AUC)\u2009=\u20090.97), smoking status (AUC\u2009=\u20090.71), systolic blood pressure (mean absolute error within 11.23\u2009mmHg) and major adverse cardiac events (AUC\u2009=\u20090.70). We also show that the trained deep-learning models used anatomical features, such as the optic disc or blood vessels, to generate each prediction.","authors":"Ryan Poplin, Avinash V. Varadarajan, Katy Blumer, Yun Liu, Michael V. McConnell, Greg S. Corrado, Lily Peng, Dale R. Webster","publication_type":"PEER-REVIEWED-JOURNAL","publications_tags":"NEW-RESEARCH","application_areas":"DIAGNOSTICS","medical_areas":"CARDIOVASCULAR-RISK-FACTORS","modalities":"FUNDUS-PHOTOGRAPHS","datasets":"EYEPACS-2K, UK-BIOBANK","patients":284335,"comment":"Landmark paper looking at the idea of deriving non-ophthalmological outcomes from retinal images","DOI":"https://doi.org/10.1038/s41551-018-0195-0","publication_date":"02/19/2018"},{"id":2,"title":"Predicting conversion to wet age-related macular degeneration using deep learning","url":"https://www.nature.com/articles/s41591-020-0867-7","download-url":"https://www.blog.google/documents/76/Full_text__41591_2020_867_OnlinePDF_2_2.pdf","abstract":"Progression to exudative \u2018wet\u2019 age-related macular degeneration (exAMD) is a major cause of visual deterioration. In patients diagnosed with exAMD in one eye, we introduce an artificial intelligence (AI) system to predict progression to exAMD in the second eye. By combining models based on three-dimensional (3D) optical coherence tomography images and corresponding automatic tissue maps, our system predicts conversion to exAMD within a clinically actionable 6-month time window, achieving a per-volumetric-scan sensitivity of 80% at 55% specificity, and 34% sensitivity at 90% specificity. This level of performance corresponds to true positives in 78% and 41% of individual eyes, and false positives in 56% and 17% of individual eyes at the high sensitivity and high specificity points, respectively. Moreover, we show that automatic tissue segmentation can identify anatomical changes before conversion and high-risk subgroups. This AI system overcomes substantial interobserver variability in expert predictions, performing better than five out of six experts, and demonstrates the potential of using AI to predict disease progression. In individuals diagnosed with age-related macular degeneration in one eye, a deep learning model can predict progression to the \u2018wet\u2019, sight-threatening form of the disease in the second eye within a 6-month time frame.","authors":"Jason Yim, Reena Chopra, Terry Spitz, Jim Winkens, Annette Obika, Christopher Kelly, Harry Askham, Marko Lukic, Josef Huemer, Katrin Fasler, Gabriella Moraes, Clemens Meyer, Marc Wilson, Jonathan Dixon, Cian Hughes, Geraint Rees, Peng T. Khaw, Alan Karthikesalingam, Dominic King, Demis Hassabis, Mustafa Suleyman, Trevor Back, Joseph R. Ledsam, Pearse A. Keane, Jeffrey De Fauw","publication_type":"PEER-REVIEWED-JOURNAL","publications_tags":"NEW-RESEARCH","application_areas":"AMD","medical_areas":"PROGNOSTICS","modalities":"OCT, SLO","datasets":"PRIVATE","patients":2795,"comment":"","DOI":"","publication_date":"05/18/2020"},{"id":3,"title":"AI papers in ophthalmology made simple","url":"https://www.nature.com/articles/s41433-020-0929-6","download-url":"https://www.nature.com/articles/s41433-020-0929-6https://www.nature.com/articles/s41433-020-0929-6.pdf","abstract":"Many general ophthalmologists may not have a computer science background, and traditional critical analysis skills for clinical studies do not always directly apply to AI studies. This editorial outlines a stepwise approach to help readers critically read the introduction, methods, results, and discussion components of an AI paper, with a view towards how these technologies can potentially be applied in routine clinical practice.","authors":"Sohee Jeon, Yun Liu, Ji-Peng Olivia Li, Dale Webster, Lily Peng, Daniel Ting","publication_type":"EDITORIAL","publications_tags":"EXPLAINER","application_areas":"","medical_areas":"","modalities":"","datasets":"","patients":"null","comment":"","DOI":"https://doi.org/10.1038/s41433-020-0929-6","publication_date":"04/22/2020"},{"id":4,"title":"Development and Validation of a Deep Learning System for Diabetic Retinopathy and Related Eye Diseases Using Retinal Images From Multiethnic Populations With Diabetes","url":"https://jamanetwork.com/article.aspx?doi=10.1001/jama.2017.18152","download-url":"","abstract":"Importance  A deep learning system (DLS) is a machine learning technology with potential for screening diabetic retinopathy and related eye diseases. Objective  To evaluate the performance of a DLS in detecting referable diabetic retinopathy, vision-threatening diabetic retinopathy, possible glaucoma, and age-related macular degeneration (AMD) in community and clinic-based multiethnic populations with diabetes. Design, Setting, and Participants  Diagnostic performance of a DLS for diabetic retinopathy and related eye diseases was evaluated using 494\u202f661 retinal images. A DLS was trained for detecting diabetic retinopathy (using 76\u202f370 images), possible glaucoma (125\u202f189 images), and AMD (72\u202f610 images), and performance of DLS was evaluated for detecting diabetic retinopathy (using 112\u202f648 images), possible glaucoma (71\u202f896 images), and AMD (35\u202f948 images). Training of the DLS was completed in May 2016, and validation of the DLS was completed in May 2017 for detection of referable diabetic retinopathy (moderate nonproliferative diabetic retinopathy or worse) and vision-threatening diabetic retinopathy (severe nonproliferative diabetic retinopathy or worse) using a primary validation data set in the Singapore National Diabetic Retinopathy Screening Program and 10 multiethnic cohorts with diabetes. Exposures  Use of a deep learning system. Main Outcomes and Measures  Area under the receiver operating characteristic curve (AUC) and sensitivity and specificity of the DLS with professional graders (retinal specialists, general ophthalmologists, trained graders, or optometrists) as the reference standard. Results  In the primary validation dataset (n\u2009=\u200914\u202f880 patients; 71\u202f896 images; mean [SD] age, 60.2 [2.2] years; 54.6% men), the prevalence of referable diabetic retinopathy was 3.0%; vision-threatening diabetic retinopathy, 0.6%; possible glaucoma, 0.1%; and AMD, 2.5%. The AUC of the DLS for referable diabetic retinopathy was 0.936 (95% CI, 0.925-0.943), sensitivity was 90.5% (95% CI, 87.3%-93.0%), and specificity was 91.6% (95% CI, 91.0%-92.2%). For vision-threatening diabetic retinopathy, AUC was 0.958 (95% CI, 0.956-0.961), sensitivity was 100% (95% CI, 94.1%-100.0%), and specificity was 91.1% (95% CI, 90.7%-91.4%). For possible glaucoma, AUC was 0.942 (95% CI, 0.929-0.954), sensitivity was 96.4% (95% CI, 81.7%-99.9%), and specificity was 87.2% (95% CI, 86.8%-87.5%). For AMD, AUC was 0.931 (95% CI, 0.928-0.935), sensitivity was 93.2% (95% CI, 91.1%-99.8%), and specificity was 88.7% (95% CI, 88.3%-89.0%). For referable diabetic retinopathy in the 10 additional datasets, AUC range was 0.889 to 0.983 (n\u2009=\u200940\u202f752 images). Conclusions and Relevance  In this evaluation of retinal images from multiethnic cohorts of patients with diabetes, the DLS had high sensitivity and specificity for identifying diabetic retinopathy and related eye diseases. Further research is necessary to evaluate the applicability of the DLS in health care settings and the utility of the DLS to improve vision outcomes.","authors":"Daniel Shu Wei Ting, Carol Yim-Lui Cheung, Gilbert Lim, Gavin Siew Wei Tan, Nguyen D. Quang, Alfred Gan, Haslina Hamzah, Renata Garcia-Franco, Ian Yew San Yeo, Shu Yen Lee, Edmund Yick Mun Wong, Charumathi Sabanayagam, Mani Baskaran, Farah Ibrahim, Ngiap Chuan Tan, Eric A. Finkelstein, Ecosse L. Lamoureux, Ian Y. Wong, Neil M. Bressler, Sobha Sivaprasad, Rohit Varma, Jost B. Jonas, Ming Guang He, Ching-Yu Cheng, Gemmy Chui Ming Cheung, Tin Aung, Wynne Hsu, Mong Li Lee, Tien Yin Wong","publication_type":"PEER-REVIEWED-JOURNAL","publications_tags":"NEW-RESEARCH","application_areas":"DIAGNOSTICS","medical_areas":"DIABETIC-RETINOPATHY, GLAUCOMA","modalities":"FUNDUS-PHOTOGRAPHS","datasets":"","patients":null,"comment":"","DOI":"","publication_date":"12/17/2017"},{"id":5,"title":"Clinical validation of an artificial intelligence-based diabetic retinopathy screening tool for a national health system","url":"https://www.nature.com/articles/s41433-020-01366-0","download-url":"","abstract":"Objective: To evaluate the accuracy and validity of an automated diabetic retinopathy (DR) screening tool (DART, TeleDx, Santiago, Chile) that uses artificial intelligence to analyze ocular fundus photographs for potential implementation in the national Chilean DR screening programme. Method: This was an observational study of 1123 diabetic eye exams using a validation protocol designed by the commission of the Chilean Ministry of Health personnel and retina specialists. Results: Receiver operating characteristic (ROC) analysis indicated a sensitivity of 94.6% (95% CI: 90.9\u201396.9%), specificity of 74.3% (95% CI: 73.3\u201375%), and negative predictive value of 98.1% (95% CI: 96.8\u201398.9%) for the automated tool at the optimal operating point for DR screening. The area under the ROC curve was 0.915. Conclusions: The results of this study suggest that DART is a valid tool that could be implemented in a heterogeneous health network such as the Chilean system.","authors":"Jos\xe9 Tom\xe1s Arenas-Cavalli, Ignacio Abarca, Maximiliano Rojas-Contreras, Fernando Bernuy, Rodrigo Donoso","publication_type":"PEER-REVIEWED-JOURNAL","publications_tags":"NEW-RESEARCH","application_areas":"","medical_areas":"DIABETIC-RETINOPATHY","modalities":"FUNDUS-PHOTOGRAPHS","datasets":"","patients":"","comment":"","DOI":"","publication_date":""},{"id":6,"title":"Modular machine learning for Alzheimer\'s disease classification from retinal vasculature","url":"https://www.nature.com/articles/s41598-020-80312-2","download-url":"https://www.nature.com/articles/s41598-020-80312-2.pdf","abstract":"Alzheimer\'s disease is the leading cause of dementia. The long progression period in Alzheimer\'s disease provides a possibility for patients to get early treatment by having routine screenings. However, current clinical diagnostic imaging tools do not meet the specific requirements for screening procedures due to high cost and limited availability. In this work, we took the initiative to evaluate the retina, especially the retinal vasculature, as an alternative for conducting screenings for dementia patients caused by Alzheimer\'s disease. Highly modular machine learning techniques were employed throughout the whole pipeline. Utilizing data from the UK Biobank, the pipeline achieved an average classification accuracy of 82.44%. Besides the high classification accuracy, we also added a saliency analysis to strengthen this pipeline\'s interpretability. The saliency analysis indicated that within retinal images, small vessels carry more information for diagnosing Alzheimer\'s diseases, which aligns with related studies.","authors":"Jianqiao Tian, Glenn Smith, Han Guo, Boya Liu, Zehua Pan, Zijie Wang, Shuangyu Xiong, Ruogu Fang","publication_type":"PEER-REVIEWED-JOURNAL","publications_tags":"NEW-RESEARCH","application_areas":"","medical_areas":"ALZHEIMER\'S","modalities":"FUNDUS-PHOTOGRAPHS","datasets":"UK BIOBANK","patients":"","comment":"","DOI":"https://doi.org/10.1038/s41598-020-80312-2","publication_date":"01/08/2021"},{"id":7,"title":"Deep learning-enabled medical computer vision","url":"https://www.nature.com/articles/s41746-020-00376-2#Sec7","download-url":"https://www.nature.com/articles/s41746-020-00376-2.pdf","abstract":"A decade of unprecedented progress in artificial intelligence (AI) has demonstrated the potential for many fields\u2014including medicine\u2014to benefit from the insights that AI techniques can extract from data. Here we survey recent progress in the development of modern computer vision techniques\u2014powered by deep learning\u2014for medical applications, focusing on medical imaging, medical video, and clinical deployment. We start by briefly summarizing a decade of progress in convolutional neural networks, including the vision tasks they enable, in the context of healthcare. Next, we discuss several example medical imaging applications that stand to benefit\u2014including cardiology, pathology, dermatology, ophthalmology\u2013and propose new avenues for continued work. We then expand into general medical video, highlighting ways in which clinical workflows can integrate computer vision to enhance care. Finally, we discuss the challenges and hurdles required for real-world clinical deployment of these technologies.","authors":"Andre Esteva, Katherine Chou, Serena Yeung, Nikhil Naik, Ali Madani, Ali Mottaghi, Yun Liu, Eric Topol, Jeff Dean, Richard Socher ","publication_type":"EDITORIAL","publications_tags":"EXPLAINER","application_areas":"","medical_areas":"","modalities":"","datasets":"","patients":"","comment":"","DOI":"https://doi.org/10.1038/s41746-020-00376-2","publication_date":"01/08/2021"}]')},87:function(e,a,i){},88:function(e,a,i){},96:function(e,a,i){"use strict";i.r(a);var t=i(4),n=i(0),s=i.n(n),o=i(26),r=i.n(o),l=(i(87),i(88),i(89),i.p+"static/media/cybersight-logo.46d41ccb.png"),c=i(111),d=i(113),h=i(112),p=i(114),u=i(64),g=i(71);var m=function(){var e=function(e){var a=e.color;return Object(t.jsx)("hr",{style:{color:a,backgroundColor:a,height:3}})};return Object(t.jsx)("div",{className:"App",children:Object(t.jsxs)(c.a,{children:[Object(t.jsxs)(d.a,{as:"h1",textAlign:"center",children:[Object(t.jsx)(h.a,{src:l}),Object(t.jsx)(d.a.Content,{children:"Awesome AI in Ophthalmology"}),Object(t.jsx)(d.a.Subheader,{children:"Powered by Cybersight-AI"})]}),Object(t.jsxs)(p.a,{padded:"very",raised:!0,size:"large",children:[Object(t.jsxs)(d.a,{as:"h3",dividing:!0,children:[Object(t.jsx)(u.a,{name:"question circle"}),Object(t.jsxs)(d.a.Content,{children:["About",Object(t.jsx)(d.a.Subheader,{children:"What is AI in Ophthalmology all about?"})]})]}),Object(t.jsxs)(p.a,{basic:!0,size:"medium",children:["Artificial Intelligence in Ophthalmology is an ever-growing field. The aim of this site is to provide a source of truth for all resources involving AI in Ophthalmology, to better understand the field, and build a community in which we can better help the world see.",Object(t.jsx)("br",{}),Object(t.jsx)(e,{color:"black"}),Object(t.jsx)(p.a,{basic:!0,size:"large",children:g.map((function(e){return Object(t.jsx)("li",{children:Object(t.jsxs)("div",{children:[Object(t.jsxs)(d.a,{as:"h3",children:[e.title,Object(t.jsx)(d.a.Subheader,{children:Object(t.jsx)("a",{href:e.url,children:"Visit Publication Here"})})]}),Object(t.jsxs)(p.a,{basic:!0,size:"small",children:["Published: ",e.publication_date]}),Object(t.jsxs)(p.a,{basic:!0,size:"medium",children:["Authors: ",e.authors]}),Object(t.jsx)(p.a,{basic:!0,size:"medium",children:e.abstract})]})},e.id)}))})]})]})]})})},b=function(e){e&&e instanceof Function&&i.e(3).then(i.bind(null,115)).then((function(a){var i=a.getCLS,t=a.getFID,n=a.getFCP,s=a.getLCP,o=a.getTTFB;i(e),t(e),n(e),s(e),o(e)}))};r.a.render(Object(t.jsx)(s.a.StrictMode,{children:Object(t.jsx)(m,{})}),document.getElementById("root")),b()}},[[96,1,2]]]);
//# sourceMappingURL=main.086ad8a7.chunk.js.map